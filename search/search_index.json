{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"mlserve mlserve is a Python library that helps you package your Machine Learning model easily into a REST API. The idea behind mlserve is to define a set of generic endpoints to make predictions easily ! Philosophy The recent surge of interest in harnessing Machine Learning to solve business problems has shed the light upon how hard it is to put it into production. Data Scientists often struggle with the technology gap between their development environment and the production environment for a lot of reasons (scalability, notebook code not ready for production, ...). The goal of mlserve is to reduce this gap by offering a simple way to package the model behind a REST API. mlserve is not the first library to help packaging of ML models behind API, but aims to be a complete one. The main goals of this library is to : Have an API that could be used by everybody (with documentation) Make the API more reliable by verifying the request's input Have an easy way to retrieve Machine Learning projects These points are mainly solved thanks to : FastAPI , as it offers a clean and eay way to define a documented API. MLflow , which helps Data Scientists store their training models and artifacts (data for example) in a generic way. Libraries supported mlserve is designed to support any Machine Learning library you want to use. As it is tightly coupled with MLflow to handle model retrieving, it supports all libraries supported by MLflow out-of-the-box.' If you want to find more on libraries supported by MLflow, go here : Libraries supported by MLflow If the library you wish to use is not supported by MLflow, do not worry, this case is covered here Requirements Python 3.6+ (for FastAPI) Installation pip install mlserve","title":"mlserve"},{"location":"#mlserve","text":"mlserve is a Python library that helps you package your Machine Learning model easily into a REST API. The idea behind mlserve is to define a set of generic endpoints to make predictions easily !","title":"mlserve"},{"location":"#philosophy","text":"The recent surge of interest in harnessing Machine Learning to solve business problems has shed the light upon how hard it is to put it into production. Data Scientists often struggle with the technology gap between their development environment and the production environment for a lot of reasons (scalability, notebook code not ready for production, ...). The goal of mlserve is to reduce this gap by offering a simple way to package the model behind a REST API. mlserve is not the first library to help packaging of ML models behind API, but aims to be a complete one. The main goals of this library is to : Have an API that could be used by everybody (with documentation) Make the API more reliable by verifying the request's input Have an easy way to retrieve Machine Learning projects These points are mainly solved thanks to : FastAPI , as it offers a clean and eay way to define a documented API. MLflow , which helps Data Scientists store their training models and artifacts (data for example) in a generic way.","title":"Philosophy"},{"location":"#libraries-supported","text":"mlserve is designed to support any Machine Learning library you want to use. As it is tightly coupled with MLflow to handle model retrieving, it supports all libraries supported by MLflow out-of-the-box.' If you want to find more on libraries supported by MLflow, go here : Libraries supported by MLflow If the library you wish to use is not supported by MLflow, do not worry, this case is covered here","title":"Libraries supported"},{"location":"#requirements","text":"Python 3.6+ (for FastAPI)","title":"Requirements"},{"location":"#installation","text":"pip install mlserve","title":"Installation"},{"location":"features/","text":"Features Machine Learning mlserve is designed to support any Machine Learning library, and is already ready to be customized as you wish. One of the main thing Data Scientists struggle with is that they do not version their models. MLflow already defines a way to store and retrieve models you trained and offers an easy way to version your models In order to expose any kind of Machine Learning model we defined two functions that might help you going faster in your deployment ! Getting a model In order to get a model, we rely heavily on MLflow, and the simplest way to do it is this way : from mlserve.loader import load_mlflow_model model = load_mlflow_model ( # MlFlow model path 'models:/sklearn_model/1' , # MlFlow Tracking URI 'http://localhost:5000' , ) This will retrieve the first version of the sklearn_model in MLflow Model Registry Note that you can use the following syntaxes for loading the model : MLflow Model Loading Syntaxes Handling the prediction Handling the API - Machine Learning Model interface can be tricky. APIs often exchange data in JSON format, whereas the main format for Machine Learning predictions is a pandas.DataFrame In order to avoid boilerplate code, we defined a GenericPrediction object that handles this interface for you. Here is an example on how to use it : from mlserve.loader import load_mlflow_model from mlserve.predictions import GenericPrediction # load model model = load_mlflow_model ( # MlFlow model path 'models:/sklearn_model/1' , # MlFlow Tracking URI 'http://localhost:5000' , ) # implement prediction methods generic_prediction = GenericPrediction ( model ) This GenericPrediction object will define : Method to transform JSON into pandas.DataFrame Method to apply prediction Method to transform prediction result into JSON API Defining the API Defining the same endpoints for every Machine Learning model you'll put into production would be painful to do every time. We defined two endpoints for you in order to avoid doing that : /predict to do a prediction based on the pre-trained model /feedback in order to get a feedback of the prediction done ( EXPERIMENTAL ) The only thing you need to do is define the type of data you will provide to the /predict endpoint : from mlserve.api import ApiBuilder from mlserve.inputs import BasicInput from mlserve.loader import load_mlflow_model from mlserve.predictions import GenericPrediction # load model model = load_mlflow_model ( # MlFlow model path 'models:/sklearn_model/1' , # MlFlow Tracking URI 'http://localhost:5000' , ) # Implement deserializer for input data class WineComposition ( BasicInput ): alcohol : float chlorides : float citric_acid : float density : float fixed_acidity : float free_sulfur_dioxide : int pH : float residual_sugar : float sulphates : float total_sulfur_dioxide : int volatile_acidity : int # implement application app = ApiBuilder ( GenericPrediction ( model ), WineComposition ) . build_api () Now you have a FastAPI application, you can run it thanks to Uvicorn . Let's say you saved the preceding script into a file named api.py , here is a simple command to run it : uvicorn api:app --host 0 .0.0.0 Using the API FastAPI defines the documentation for you and provides two interfaces for that ReDoc You can access it on http://localhost:8000/redoc OpenAPI You can access it on http://localhost:8000/docs","title":"Features"},{"location":"features/#features","text":"","title":"Features"},{"location":"features/#machine-learning","text":"mlserve is designed to support any Machine Learning library, and is already ready to be customized as you wish. One of the main thing Data Scientists struggle with is that they do not version their models. MLflow already defines a way to store and retrieve models you trained and offers an easy way to version your models In order to expose any kind of Machine Learning model we defined two functions that might help you going faster in your deployment !","title":"Machine Learning"},{"location":"features/#getting-a-model","text":"In order to get a model, we rely heavily on MLflow, and the simplest way to do it is this way : from mlserve.loader import load_mlflow_model model = load_mlflow_model ( # MlFlow model path 'models:/sklearn_model/1' , # MlFlow Tracking URI 'http://localhost:5000' , ) This will retrieve the first version of the sklearn_model in MLflow Model Registry Note that you can use the following syntaxes for loading the model : MLflow Model Loading Syntaxes","title":"Getting a model"},{"location":"features/#handling-the-prediction","text":"Handling the API - Machine Learning Model interface can be tricky. APIs often exchange data in JSON format, whereas the main format for Machine Learning predictions is a pandas.DataFrame In order to avoid boilerplate code, we defined a GenericPrediction object that handles this interface for you. Here is an example on how to use it : from mlserve.loader import load_mlflow_model from mlserve.predictions import GenericPrediction # load model model = load_mlflow_model ( # MlFlow model path 'models:/sklearn_model/1' , # MlFlow Tracking URI 'http://localhost:5000' , ) # implement prediction methods generic_prediction = GenericPrediction ( model ) This GenericPrediction object will define : Method to transform JSON into pandas.DataFrame Method to apply prediction Method to transform prediction result into JSON","title":"Handling the prediction"},{"location":"features/#api","text":"","title":"API"},{"location":"features/#defining-the-api","text":"Defining the same endpoints for every Machine Learning model you'll put into production would be painful to do every time. We defined two endpoints for you in order to avoid doing that : /predict to do a prediction based on the pre-trained model /feedback in order to get a feedback of the prediction done ( EXPERIMENTAL ) The only thing you need to do is define the type of data you will provide to the /predict endpoint : from mlserve.api import ApiBuilder from mlserve.inputs import BasicInput from mlserve.loader import load_mlflow_model from mlserve.predictions import GenericPrediction # load model model = load_mlflow_model ( # MlFlow model path 'models:/sklearn_model/1' , # MlFlow Tracking URI 'http://localhost:5000' , ) # Implement deserializer for input data class WineComposition ( BasicInput ): alcohol : float chlorides : float citric_acid : float density : float fixed_acidity : float free_sulfur_dioxide : int pH : float residual_sugar : float sulphates : float total_sulfur_dioxide : int volatile_acidity : int # implement application app = ApiBuilder ( GenericPrediction ( model ), WineComposition ) . build_api () Now you have a FastAPI application, you can run it thanks to Uvicorn . Let's say you saved the preceding script into a file named api.py , here is a simple command to run it : uvicorn api:app --host 0 .0.0.0","title":"Defining the API"},{"location":"features/#using-the-api","text":"FastAPI defines the documentation for you and provides two interfaces for that","title":"Using the API"},{"location":"features/#redoc","text":"You can access it on http://localhost:8000/redoc","title":"ReDoc"},{"location":"features/#openapi","text":"You can access it on http://localhost:8000/docs","title":"OpenAPI"},{"location":"going_further/","text":"FastAPI configuration By default, we do not change the FastAPI configuration, so you end up with a generic configuration. One way to avoid that is to define a configuration file (for example fastapi.cfg ) like this : [fastapi] title = WineQualityApi description = This API helps you determine if the quality of the Wine is good or not version = 0.1.0 And then to give it to your application : from mlserve.api import ApiBuilder from mlserve.inputs import BasicInput from mlserve.loader import load_mlflow_model from mlserve.predictions import GenericPrediction # load model model = load_mlflow_model ( # MlFlow model path 'models:/sklearn_model/1' , # MlFlow Tracking URI 'http://localhost:5000' , ) # Implement deserializer for input data class WineComposition ( BasicInput ): alcohol : float chlorides : float citric_acid : float density : float fixed_acidity : float free_sulfur_dioxide : int pH : float residual_sugar : float sulphates : float total_sulfur_dioxide : int volatile_acidity : int # implement application app = ApiBuilder ( GenericPrediction ( model ), WineComposition , configuration_path = 'fastapi.cfg' ) . build_api () Defining your own prediction object The GenericPrediction object implements the AbstractPrediction class in order to ease the interface between the API and the model prediction. Under the hood, the GenericPrediction does a little a bit more than that, in order : _transforms_input : Transforms JSON into pandas.DataFrame _fetch_data : Fetch data from an external source (by default does nothing) _combine_fetched_data_with_input : Combine fetched data with pandas.DataFrame (by default only returns pandas.DataFrame ) _apply_model : Apply model _transform_output : Transforms result (either pandas.DataFrame , numpy.ndarray , pandas.Series ) into JSON Retrieving additional data before applying model Let's say that the data given in input of the API is not enough, and you need additional information in order to make your prediction. This could be done easily by overriding these two functions in the GenericPrediction class (inherited by the AbstractPrediction class): import logging from abc import ABC , abstractmethod import numpy as np import pandas as pd from mlserve.inputs import BasicInput from mlserve.utils import pydantic_model_to_pandas , pandas_to_dict class AbstractPrediction ( ABC ): \"\"\" Abstract class to define methods called during predict \"\"\" def __init__ ( self , model ): self . model = model @abstractmethod def _transform_input ( self , input ): \"\"\" Function called right after API call. It is supposed to transform <pydantic.BaseModel> object into the input data format needed to apply model \"\"\" @staticmethod def _fetch_data ( input : BasicInput ): \"\"\" Helper function in case we need additional data. In most of the cases, can be ignored \"\"\" pass @staticmethod def _combine_fetched_data_with_input ( fetched_data , transformed_input ): return transformed_input @abstractmethod def _apply_model ( self , transformed_input ): \"\"\" Function called to apply Machine Learning model to predict from the transformed input \"\"\" @abstractmethod def _transform_output ( self , output ): \"\"\" Function called right after applying model to input data. Supposed to transform the data that we got after the predict in order to \"\"\" def predict ( self , input , uuid ): \"\"\" Main function that will be used by all the childs to apply model. Here are the steps made : - Transform <pydantic.BaseModel> objet to target input object before applying model - Apply model - Transform output into more suitable format for an API - Add an uuid to the request to track request made \"\"\" logging . debug ( \"Got input: {} for request_id: {} \" . format ( input . dict (), uuid ) ) logging . info ( \"Transforming input for request: {} \" . format ( uuid )) transformed_input = self . _transform_input ( input ) logging . debug ( \"Input transformed to this: {} \" . format ( transformed_input ) ) logging . info ( \"Fetching data for request: {} \" . format ( uuid )) fetched_data = self . _fetch_data ( input ) logging . debug ( \"Fetched data: {} \" . format ( fetched_data )) logging . info ( \"Combining fetched data and input for request {} \" . format ( uuid ) ) combined_data = self . _combine_fetched_data_with_input ( fetched_data , transformed_input ) logging . debug ( \"Combined data: {} \" . format ( combined_data )) logging . info ( \"Applying input for request {} \" . format ( uuid )) output = self . _apply_model ( combined_data ) logging . debug ( \"Prediction output: {} \" . format ( output )) logging . info ( \"Transforming output for request {} \" . format ( uuid )) transformed_output = self . _transform_output ( output ) logging . debug ( \"Transformed output: {} \" . format ( transformed_output )) transformed_output [ \"request_id\" ] = uuid return transformed_output class GenericPrediction ( AbstractPrediction ): \"\"\" Implementation of <mlserve.ml.model.AbstractModel> for scikit-learn \"\"\" def _transform_input ( self , input ) -> pd . DataFrame : \"\"\" Transforms <pydantic.BaseModel> object to <pandas.DataFrame> \"\"\" return pydantic_model_to_pandas ( input ) def _apply_model ( self , transformed_input : pd . DataFrame ): \"\"\" Applies the sklearn model to the <pandas.DataFrame>. Returns either one of these: - <pandas.DataFrame> - <pandas.Series> - <numpy.ndarray> \"\"\" return self . model . predict ( transformed_input ) def _transform_output ( self , output ) -> dict : \"\"\" Transforms output given by <mlserve.ml.sklearn._apply_model> to prepare sending result with API. \"\"\" if isinstance ( output , np . ndarray ): result = output . tolist () elif isinstance ( output , pd . DataFrame ): result = pandas_to_dict ( output ) elif isinstance ( output , pd . Series ): result = output . to_dict () else : result = None return { \"result\" : result }","title":"Going further"},{"location":"going_further/#fastapi-configuration","text":"By default, we do not change the FastAPI configuration, so you end up with a generic configuration. One way to avoid that is to define a configuration file (for example fastapi.cfg ) like this : [fastapi] title = WineQualityApi description = This API helps you determine if the quality of the Wine is good or not version = 0.1.0 And then to give it to your application : from mlserve.api import ApiBuilder from mlserve.inputs import BasicInput from mlserve.loader import load_mlflow_model from mlserve.predictions import GenericPrediction # load model model = load_mlflow_model ( # MlFlow model path 'models:/sklearn_model/1' , # MlFlow Tracking URI 'http://localhost:5000' , ) # Implement deserializer for input data class WineComposition ( BasicInput ): alcohol : float chlorides : float citric_acid : float density : float fixed_acidity : float free_sulfur_dioxide : int pH : float residual_sugar : float sulphates : float total_sulfur_dioxide : int volatile_acidity : int # implement application app = ApiBuilder ( GenericPrediction ( model ), WineComposition , configuration_path = 'fastapi.cfg' ) . build_api ()","title":"FastAPI configuration"},{"location":"going_further/#defining-your-own-prediction-object","text":"The GenericPrediction object implements the AbstractPrediction class in order to ease the interface between the API and the model prediction. Under the hood, the GenericPrediction does a little a bit more than that, in order : _transforms_input : Transforms JSON into pandas.DataFrame _fetch_data : Fetch data from an external source (by default does nothing) _combine_fetched_data_with_input : Combine fetched data with pandas.DataFrame (by default only returns pandas.DataFrame ) _apply_model : Apply model _transform_output : Transforms result (either pandas.DataFrame , numpy.ndarray , pandas.Series ) into JSON","title":"Defining your own prediction object"},{"location":"going_further/#retrieving-additional-data-before-applying-model","text":"Let's say that the data given in input of the API is not enough, and you need additional information in order to make your prediction. This could be done easily by overriding these two functions in the GenericPrediction class (inherited by the AbstractPrediction class): import logging from abc import ABC , abstractmethod import numpy as np import pandas as pd from mlserve.inputs import BasicInput from mlserve.utils import pydantic_model_to_pandas , pandas_to_dict class AbstractPrediction ( ABC ): \"\"\" Abstract class to define methods called during predict \"\"\" def __init__ ( self , model ): self . model = model @abstractmethod def _transform_input ( self , input ): \"\"\" Function called right after API call. It is supposed to transform <pydantic.BaseModel> object into the input data format needed to apply model \"\"\" @staticmethod def _fetch_data ( input : BasicInput ): \"\"\" Helper function in case we need additional data. In most of the cases, can be ignored \"\"\" pass @staticmethod def _combine_fetched_data_with_input ( fetched_data , transformed_input ): return transformed_input @abstractmethod def _apply_model ( self , transformed_input ): \"\"\" Function called to apply Machine Learning model to predict from the transformed input \"\"\" @abstractmethod def _transform_output ( self , output ): \"\"\" Function called right after applying model to input data. Supposed to transform the data that we got after the predict in order to \"\"\" def predict ( self , input , uuid ): \"\"\" Main function that will be used by all the childs to apply model. Here are the steps made : - Transform <pydantic.BaseModel> objet to target input object before applying model - Apply model - Transform output into more suitable format for an API - Add an uuid to the request to track request made \"\"\" logging . debug ( \"Got input: {} for request_id: {} \" . format ( input . dict (), uuid ) ) logging . info ( \"Transforming input for request: {} \" . format ( uuid )) transformed_input = self . _transform_input ( input ) logging . debug ( \"Input transformed to this: {} \" . format ( transformed_input ) ) logging . info ( \"Fetching data for request: {} \" . format ( uuid )) fetched_data = self . _fetch_data ( input ) logging . debug ( \"Fetched data: {} \" . format ( fetched_data )) logging . info ( \"Combining fetched data and input for request {} \" . format ( uuid ) ) combined_data = self . _combine_fetched_data_with_input ( fetched_data , transformed_input ) logging . debug ( \"Combined data: {} \" . format ( combined_data )) logging . info ( \"Applying input for request {} \" . format ( uuid )) output = self . _apply_model ( combined_data ) logging . debug ( \"Prediction output: {} \" . format ( output )) logging . info ( \"Transforming output for request {} \" . format ( uuid )) transformed_output = self . _transform_output ( output ) logging . debug ( \"Transformed output: {} \" . format ( transformed_output )) transformed_output [ \"request_id\" ] = uuid return transformed_output class GenericPrediction ( AbstractPrediction ): \"\"\" Implementation of <mlserve.ml.model.AbstractModel> for scikit-learn \"\"\" def _transform_input ( self , input ) -> pd . DataFrame : \"\"\" Transforms <pydantic.BaseModel> object to <pandas.DataFrame> \"\"\" return pydantic_model_to_pandas ( input ) def _apply_model ( self , transformed_input : pd . DataFrame ): \"\"\" Applies the sklearn model to the <pandas.DataFrame>. Returns either one of these: - <pandas.DataFrame> - <pandas.Series> - <numpy.ndarray> \"\"\" return self . model . predict ( transformed_input ) def _transform_output ( self , output ) -> dict : \"\"\" Transforms output given by <mlserve.ml.sklearn._apply_model> to prepare sending result with API. \"\"\" if isinstance ( output , np . ndarray ): result = output . tolist () elif isinstance ( output , pd . DataFrame ): result = pandas_to_dict ( output ) elif isinstance ( output , pd . Series ): result = output . to_dict () else : result = None return { \"result\" : result }","title":"Retrieving additional data before applying model"},{"location":"examples/","text":"Setup To run the examples of the libraries we put here, we strongly recommend using Docker First, let's clone the repository, build the docker container and enter it: git clone https://github.com/gfalcone/mlserve cd mlserve docker build --tag = mlserve . docker run -ti -p 8000 :8000 mlserve bash Now we can train our models and serve them !","title":"Examples - Introduction"},{"location":"examples/#setup","text":"To run the examples of the libraries we put here, we strongly recommend using Docker First, let's clone the repository, build the docker container and enter it: git clone https://github.com/gfalcone/mlserve cd mlserve docker build --tag = mlserve . docker run -ti -p 8000 :8000 mlserve bash Now we can train our models and serve them !","title":"Setup"},{"location":"examples/keras/","text":"Training \"\"\" Example taken from https://github.com/mlflow/mlflow/blob/master/examples/keras/train.py \"\"\" from __future__ import print_function import numpy as np import keras from keras.datasets import reuters from keras.models import Sequential from keras.layers import Dense , Dropout , Activation from keras.preprocessing.text import Tokenizer # The following import and function call are the only additions to code required # to automatically log metrics and parameters to MLflow. import mlflow.keras mlflow . keras . autolog () max_words = 1000 batch_size = 32 epochs = 5 print ( \"Loading data...\" ) ( x_train , y_train ), ( x_test , y_test ) = reuters . load_data ( num_words = max_words , test_split = 0.2 ) print ( len ( x_train ), \"train sequences\" ) print ( len ( x_test ), \"test sequences\" ) num_classes = np . max ( y_train ) + 1 print ( num_classes , \"classes\" ) print ( \"Vectorizing sequence data...\" ) tokenizer = Tokenizer ( num_words = max_words ) x_train = tokenizer . sequences_to_matrix ( x_train , mode = \"binary\" ) x_test = tokenizer . sequences_to_matrix ( x_test , mode = \"binary\" ) print ( \"x_train shape:\" , x_train . shape ) print ( \"x_test shape:\" , x_test . shape ) print ( \"Convert class vector to binary class matrix \" \"(for use with categorical_crossentropy)\" ) y_train = keras . utils . to_categorical ( y_train , num_classes ) y_test = keras . utils . to_categorical ( y_test , num_classes ) print ( \"y_train shape:\" , y_train . shape ) print ( \"y_test shape:\" , y_test . shape ) print ( \"Building model...\" ) with mlflow . start_run ( experiment_id = 2 ) as run : model = Sequential () model . add ( Dense ( 512 , input_shape = ( max_words ,))) model . add ( Activation ( \"relu\" )) model . add ( Dropout ( 0.5 )) model . add ( Dense ( num_classes )) model . add ( Activation ( \"softmax\" )) model . compile ( loss = \"categorical_crossentropy\" , optimizer = \"adam\" , metrics = [ \"accuracy\" ] ) history = model . fit ( x_train , y_train , batch_size = batch_size , epochs = epochs , verbose = 1 , validation_split = 0.1 , ) score = model . evaluate ( x_test , y_test , batch_size = batch_size , verbose = 1 ) print ( \"Test score:\" , score [ 0 ]) print ( \"Test accuracy:\" , score [ 1 ]) mlflow . keras . log_model ( model , \"model\" , registered_model_name = \"keras_model\" ) To run it : python3 -m examples.training.keras Serving from typing import List import pandas as pd from keras.preprocessing.text import Tokenizer from mlserve.api import ApiBuilder from mlserve.inputs import BasicInput from mlserve.loader import load_mlflow_model from mlserve.predictions import GenericPrediction # load model model = load_mlflow_model ( # MlFlow model path \"models:/keras_model/1\" , # MlFlow Tracking URI \"http://localhost:5000\" , ) # Implement deserializer for input data class ReutersNewswireTopic ( BasicInput ): sequence : List [ int ] # Implement prediction because this is a bit custom class CustomKerasApplication ( GenericPrediction ): def _transform_input ( self , input : ReutersNewswireTopic ): \"\"\" Transforms <ReutersNewswireTopic> object to <numpy.array> \"\"\" max_words = 1000 tokenizer = Tokenizer ( num_words = max_words ) x_train = tokenizer . sequences_to_matrix ( [ input . sequence ], mode = \"binary\" ) return pd . DataFrame ( x_train ) # implement application app = ApiBuilder ( CustomKerasApplication ( model ), ReutersNewswireTopic ) . build_api () To run it : uvicorn examples.serving.keras:app --host 0 .0.0.0","title":"Keras"},{"location":"examples/keras/#training","text":"\"\"\" Example taken from https://github.com/mlflow/mlflow/blob/master/examples/keras/train.py \"\"\" from __future__ import print_function import numpy as np import keras from keras.datasets import reuters from keras.models import Sequential from keras.layers import Dense , Dropout , Activation from keras.preprocessing.text import Tokenizer # The following import and function call are the only additions to code required # to automatically log metrics and parameters to MLflow. import mlflow.keras mlflow . keras . autolog () max_words = 1000 batch_size = 32 epochs = 5 print ( \"Loading data...\" ) ( x_train , y_train ), ( x_test , y_test ) = reuters . load_data ( num_words = max_words , test_split = 0.2 ) print ( len ( x_train ), \"train sequences\" ) print ( len ( x_test ), \"test sequences\" ) num_classes = np . max ( y_train ) + 1 print ( num_classes , \"classes\" ) print ( \"Vectorizing sequence data...\" ) tokenizer = Tokenizer ( num_words = max_words ) x_train = tokenizer . sequences_to_matrix ( x_train , mode = \"binary\" ) x_test = tokenizer . sequences_to_matrix ( x_test , mode = \"binary\" ) print ( \"x_train shape:\" , x_train . shape ) print ( \"x_test shape:\" , x_test . shape ) print ( \"Convert class vector to binary class matrix \" \"(for use with categorical_crossentropy)\" ) y_train = keras . utils . to_categorical ( y_train , num_classes ) y_test = keras . utils . to_categorical ( y_test , num_classes ) print ( \"y_train shape:\" , y_train . shape ) print ( \"y_test shape:\" , y_test . shape ) print ( \"Building model...\" ) with mlflow . start_run ( experiment_id = 2 ) as run : model = Sequential () model . add ( Dense ( 512 , input_shape = ( max_words ,))) model . add ( Activation ( \"relu\" )) model . add ( Dropout ( 0.5 )) model . add ( Dense ( num_classes )) model . add ( Activation ( \"softmax\" )) model . compile ( loss = \"categorical_crossentropy\" , optimizer = \"adam\" , metrics = [ \"accuracy\" ] ) history = model . fit ( x_train , y_train , batch_size = batch_size , epochs = epochs , verbose = 1 , validation_split = 0.1 , ) score = model . evaluate ( x_test , y_test , batch_size = batch_size , verbose = 1 ) print ( \"Test score:\" , score [ 0 ]) print ( \"Test accuracy:\" , score [ 1 ]) mlflow . keras . log_model ( model , \"model\" , registered_model_name = \"keras_model\" ) To run it : python3 -m examples.training.keras","title":"Training"},{"location":"examples/keras/#serving","text":"from typing import List import pandas as pd from keras.preprocessing.text import Tokenizer from mlserve.api import ApiBuilder from mlserve.inputs import BasicInput from mlserve.loader import load_mlflow_model from mlserve.predictions import GenericPrediction # load model model = load_mlflow_model ( # MlFlow model path \"models:/keras_model/1\" , # MlFlow Tracking URI \"http://localhost:5000\" , ) # Implement deserializer for input data class ReutersNewswireTopic ( BasicInput ): sequence : List [ int ] # Implement prediction because this is a bit custom class CustomKerasApplication ( GenericPrediction ): def _transform_input ( self , input : ReutersNewswireTopic ): \"\"\" Transforms <ReutersNewswireTopic> object to <numpy.array> \"\"\" max_words = 1000 tokenizer = Tokenizer ( num_words = max_words ) x_train = tokenizer . sequences_to_matrix ( [ input . sequence ], mode = \"binary\" ) return pd . DataFrame ( x_train ) # implement application app = ApiBuilder ( CustomKerasApplication ( model ), ReutersNewswireTopic ) . build_api () To run it : uvicorn examples.serving.keras:app --host 0 .0.0.0","title":"Serving"},{"location":"examples/prophet/","text":"Training \"\"\" Example taken from https://github.com/mlflow/mlflow/blob/master/examples/prophet/train.py \"\"\" import warnings import sys import pandas as pd import numpy as np import mlflow import mlflow.pyfunc import cloudpickle import fbprophet from fbprophet import Prophet from fbprophet.diagnostics import cross_validation from fbprophet.diagnostics import performance_metrics import logging logging . basicConfig ( level = logging . WARN ) logger = logging . getLogger ( __name__ ) class FbProphetWrapper ( mlflow . pyfunc . PythonModel ): def __init__ ( self , model ): self . model = model super ( FbProphetWrapper , self ) . __init__ () def load_context ( self , context ): from fbprophet import Prophet return def predict ( self , context , model_input ): future = self . model . make_future_dataframe ( periods = model_input [ \"periods\" ][ 0 ] ) return self . model . predict ( future ) conda_env = { \"channels\" : [ \"defaults\" , \"conda-forge\" ], \"dependencies\" : [ \"fbprophet= {} \" . format ( fbprophet . __version__ ), \"cloudpickle= {} \" . format ( cloudpickle . __version__ ), ], \"name\" : \"fbp_env\" , } if __name__ == \"__main__\" : warnings . filterwarnings ( \"ignore\" ) np . random . seed ( 40 ) csv_url = ( sys . argv [ 1 ] if len ( sys . argv ) > 1 else \"https://raw.githubusercontent.com/facebook/prophet/e21a05f4f9290649255a2a306855e8b4620816d7/examples/example_wp_log_peyton_manning.csv\" ) rolling_window = float ( sys . argv [ 2 ]) if len ( sys . argv ) > 2 else 0.1 # Read the csv file from the URL try : df = pd . read_csv ( csv_url ) except Exception as e : logger . exception ( \"Unable to download training & test CSV, check your internet connection. Error: %s \" , e , ) # Useful for multiple runs (only doing one run in this sample notebook) with mlflow . start_run ( experiment_id = 5 ): m = Prophet () m . fit ( df ) # Evaluate Metrics df_cv = cross_validation ( m , initial = \"730 days\" , period = \"180 days\" , horizon = \"365 days\" ) df_p = performance_metrics ( df_cv , rolling_window = rolling_window ) # Print out metrics print ( \"Prophet model (rolling_window= %f ):\" % ( rolling_window )) print ( \" CV: \\n %s \" % df_cv . head ()) print ( \" Perf: \\n %s \" % df_p . head ()) # Log parameter, metrics, and model to MLflow mlflow . log_param ( \"rolling_window\" , rolling_window ) mlflow . log_metric ( \"rmse\" , df_p . loc [ 0 , \"rmse\" ]) mlflow . pyfunc . log_model ( \"model\" , conda_env = conda_env , python_model = FbProphetWrapper ( m ), registered_model_name = \"prophet_model\" , ) print ( \"Logged model with URI: runs:/ {run_id} /model\" . format ( run_id = mlflow . active_run () . info . run_id ) ) To run it : python3 -m examples.training.prophet Serving from mlserve.api import ApiBuilder from mlserve.inputs import BasicInput from mlserve.loader import load_mlflow_model from mlserve.predictions import GenericPrediction # load model model = load_mlflow_model ( # MlFlow model path \"models:/prophet_model/1\" , # MlFlow Tracking URI \"http://localhost:5000\" , ) # Implement deserializer for input data class PeriodPrediction ( BasicInput ): periods : int # implement application app = ApiBuilder ( GenericPrediction ( model ), PeriodPrediction ) . build_api () To run it : uvicorn examples.serving.prophet:app --host 0 .0.0.0","title":"Prophet"},{"location":"examples/prophet/#training","text":"\"\"\" Example taken from https://github.com/mlflow/mlflow/blob/master/examples/prophet/train.py \"\"\" import warnings import sys import pandas as pd import numpy as np import mlflow import mlflow.pyfunc import cloudpickle import fbprophet from fbprophet import Prophet from fbprophet.diagnostics import cross_validation from fbprophet.diagnostics import performance_metrics import logging logging . basicConfig ( level = logging . WARN ) logger = logging . getLogger ( __name__ ) class FbProphetWrapper ( mlflow . pyfunc . PythonModel ): def __init__ ( self , model ): self . model = model super ( FbProphetWrapper , self ) . __init__ () def load_context ( self , context ): from fbprophet import Prophet return def predict ( self , context , model_input ): future = self . model . make_future_dataframe ( periods = model_input [ \"periods\" ][ 0 ] ) return self . model . predict ( future ) conda_env = { \"channels\" : [ \"defaults\" , \"conda-forge\" ], \"dependencies\" : [ \"fbprophet= {} \" . format ( fbprophet . __version__ ), \"cloudpickle= {} \" . format ( cloudpickle . __version__ ), ], \"name\" : \"fbp_env\" , } if __name__ == \"__main__\" : warnings . filterwarnings ( \"ignore\" ) np . random . seed ( 40 ) csv_url = ( sys . argv [ 1 ] if len ( sys . argv ) > 1 else \"https://raw.githubusercontent.com/facebook/prophet/e21a05f4f9290649255a2a306855e8b4620816d7/examples/example_wp_log_peyton_manning.csv\" ) rolling_window = float ( sys . argv [ 2 ]) if len ( sys . argv ) > 2 else 0.1 # Read the csv file from the URL try : df = pd . read_csv ( csv_url ) except Exception as e : logger . exception ( \"Unable to download training & test CSV, check your internet connection. Error: %s \" , e , ) # Useful for multiple runs (only doing one run in this sample notebook) with mlflow . start_run ( experiment_id = 5 ): m = Prophet () m . fit ( df ) # Evaluate Metrics df_cv = cross_validation ( m , initial = \"730 days\" , period = \"180 days\" , horizon = \"365 days\" ) df_p = performance_metrics ( df_cv , rolling_window = rolling_window ) # Print out metrics print ( \"Prophet model (rolling_window= %f ):\" % ( rolling_window )) print ( \" CV: \\n %s \" % df_cv . head ()) print ( \" Perf: \\n %s \" % df_p . head ()) # Log parameter, metrics, and model to MLflow mlflow . log_param ( \"rolling_window\" , rolling_window ) mlflow . log_metric ( \"rmse\" , df_p . loc [ 0 , \"rmse\" ]) mlflow . pyfunc . log_model ( \"model\" , conda_env = conda_env , python_model = FbProphetWrapper ( m ), registered_model_name = \"prophet_model\" , ) print ( \"Logged model with URI: runs:/ {run_id} /model\" . format ( run_id = mlflow . active_run () . info . run_id ) ) To run it : python3 -m examples.training.prophet","title":"Training"},{"location":"examples/prophet/#serving","text":"from mlserve.api import ApiBuilder from mlserve.inputs import BasicInput from mlserve.loader import load_mlflow_model from mlserve.predictions import GenericPrediction # load model model = load_mlflow_model ( # MlFlow model path \"models:/prophet_model/1\" , # MlFlow Tracking URI \"http://localhost:5000\" , ) # Implement deserializer for input data class PeriodPrediction ( BasicInput ): periods : int # implement application app = ApiBuilder ( GenericPrediction ( model ), PeriodPrediction ) . build_api () To run it : uvicorn examples.serving.prophet:app --host 0 .0.0.0","title":"Serving"},{"location":"examples/pytorch/","text":"Training \"\"\" Example taken from https://github.com/mlflow/mlflow/blob/master/mlflow/pytorch/__init__.py \"\"\" import torch import mlflow import mlflow.pytorch # X data x_data = torch . Tensor ([[ 1.0 ], [ 2.0 ], [ 3.0 ]]) # Y data with its expected value: labels y_data = torch . Tensor ([[ 2.0 ], [ 4.0 ], [ 6.0 ]]) # Partial Model example modified from Sung Kim # https://github.com/hunkim/PyTorchZeroToAll class Model ( torch . nn . Module ): def __init__ ( self ): super ( Model , self ) . __init__ () self . linear = torch . nn . Linear ( 1 , 1 ) # One in and one out def forward ( self , x ): y_pred = self . linear ( x ) return y_pred # our model model = Model () criterion = torch . nn . MSELoss ( size_average = False ) optimizer = torch . optim . SGD ( model . parameters (), lr = 0.01 ) # Training loop for epoch in range ( 500 ): # Forward pass: Compute predicted y by passing x to the model y_pred = model ( x_data ) # Compute and print loss loss = criterion ( y_pred , y_data ) # Zero gradients, perform a backward pass, and update the weights. optimizer . zero_grad () loss . backward () optimizer . step () # After training for hv in [ 4.0 , 5.0 , 6.0 ]: hour_var = torch . Tensor ([[ hv ]]) y_pred = model ( hour_var ) # log the model with mlflow . start_run ( experiment_id = 1 ) as run : mlflow . log_param ( \"epochs\" , 500 ) mlflow . pytorch . log_model ( model , \"model\" , registered_model_name = \"pytorch_model\" ) To run it : python3 -m examples.training.pytorch Serving from mlserve.api import ApiBuilder from mlserve.inputs import BasicInput from mlserve.loader import load_mlflow_model from mlserve.predictions import GenericPrediction # load model model = load_mlflow_model ( # MlFlow model path \"models:/pytorch_model/1\" , # MlFlow Tracking URI \"http://localhost:5000\" , ) # Implement deserializer for input data class LinearRegression ( BasicInput ): input_prediction : float # implement application app = ApiBuilder ( GenericPrediction ( model ), LinearRegression ) . build_api () To run it : uvicorn examples.serving.pytorch:app --host 0 .0.0.0","title":"Pytorch"},{"location":"examples/pytorch/#training","text":"\"\"\" Example taken from https://github.com/mlflow/mlflow/blob/master/mlflow/pytorch/__init__.py \"\"\" import torch import mlflow import mlflow.pytorch # X data x_data = torch . Tensor ([[ 1.0 ], [ 2.0 ], [ 3.0 ]]) # Y data with its expected value: labels y_data = torch . Tensor ([[ 2.0 ], [ 4.0 ], [ 6.0 ]]) # Partial Model example modified from Sung Kim # https://github.com/hunkim/PyTorchZeroToAll class Model ( torch . nn . Module ): def __init__ ( self ): super ( Model , self ) . __init__ () self . linear = torch . nn . Linear ( 1 , 1 ) # One in and one out def forward ( self , x ): y_pred = self . linear ( x ) return y_pred # our model model = Model () criterion = torch . nn . MSELoss ( size_average = False ) optimizer = torch . optim . SGD ( model . parameters (), lr = 0.01 ) # Training loop for epoch in range ( 500 ): # Forward pass: Compute predicted y by passing x to the model y_pred = model ( x_data ) # Compute and print loss loss = criterion ( y_pred , y_data ) # Zero gradients, perform a backward pass, and update the weights. optimizer . zero_grad () loss . backward () optimizer . step () # After training for hv in [ 4.0 , 5.0 , 6.0 ]: hour_var = torch . Tensor ([[ hv ]]) y_pred = model ( hour_var ) # log the model with mlflow . start_run ( experiment_id = 1 ) as run : mlflow . log_param ( \"epochs\" , 500 ) mlflow . pytorch . log_model ( model , \"model\" , registered_model_name = \"pytorch_model\" ) To run it : python3 -m examples.training.pytorch","title":"Training"},{"location":"examples/pytorch/#serving","text":"from mlserve.api import ApiBuilder from mlserve.inputs import BasicInput from mlserve.loader import load_mlflow_model from mlserve.predictions import GenericPrediction # load model model = load_mlflow_model ( # MlFlow model path \"models:/pytorch_model/1\" , # MlFlow Tracking URI \"http://localhost:5000\" , ) # Implement deserializer for input data class LinearRegression ( BasicInput ): input_prediction : float # implement application app = ApiBuilder ( GenericPrediction ( model ), LinearRegression ) . build_api () To run it : uvicorn examples.serving.pytorch:app --host 0 .0.0.0","title":"Serving"},{"location":"examples/sklearn/","text":"Training \"\"\" Example taken from https://github.com/mlflow/mlflow/blob/master/examples/sklearn_elasticnet_wine/train.py \"\"\" import warnings import sys import pandas as pd import numpy as np from sklearn.metrics import mean_squared_error , mean_absolute_error , r2_score from sklearn.model_selection import train_test_split from sklearn.linear_model import ElasticNet import mlflow import mlflow.sklearn import logging logging . basicConfig ( level = logging . WARN ) logger = logging . getLogger ( __name__ ) def eval_metrics ( actual , pred ): rmse = np . sqrt ( mean_squared_error ( actual , pred )) mae = mean_absolute_error ( actual , pred ) r2 = r2_score ( actual , pred ) return rmse , mae , r2 if __name__ == \"__main__\" : mlflow . set_tracking_uri ( \"http://localhost:5000\" ) warnings . filterwarnings ( \"ignore\" ) np . random . seed ( 40 ) # Read the wine-quality csv file from the URL csv_url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\" try : data = pd . read_csv ( csv_url , sep = \";\" ) except Exception as e : logger . exception ( \"Unable to download training & test CSV, check your internet connection. Error: %s \" , e , ) # Split the data into training and test sets. (0.75, 0.25) split. train , test = train_test_split ( data ) # The predicted column is \"quality\" which is a scalar from [3, 9] train_x = train . drop ([ \"quality\" ], axis = 1 ) test_x = test . drop ([ \"quality\" ], axis = 1 ) train_y = train [[ \"quality\" ]] test_y = test [[ \"quality\" ]] alpha = float ( sys . argv [ 1 ]) if len ( sys . argv ) > 1 else 0.5 l1_ratio = float ( sys . argv [ 2 ]) if len ( sys . argv ) > 2 else 0.5 with mlflow . start_run ( experiment_id = 0 ): lr = ElasticNet ( alpha = alpha , l1_ratio = l1_ratio , random_state = 42 ) lr . fit ( train_x , train_y ) predicted_qualities = lr . predict ( test_x ) ( rmse , mae , r2 ) = eval_metrics ( test_y , predicted_qualities ) print ( \"Elasticnet model (alpha= %f , l1_ratio= %f ):\" % ( alpha , l1_ratio )) print ( \" RMSE: %s \" % rmse ) print ( \" MAE: %s \" % mae ) print ( \" R2: %s \" % r2 ) mlflow . log_param ( \"alpha\" , alpha ) mlflow . log_param ( \"l1_ratio\" , l1_ratio ) mlflow . log_metric ( \"rmse\" , rmse ) mlflow . log_metric ( \"r2\" , r2 ) mlflow . log_metric ( \"mae\" , mae ) mlflow . sklearn . log_model ( lr , \"model\" , registered_model_name = \"sklearn_model\" ) To run it : python3 -m examples.training.sklearn Serving from mlserve.api import ApiBuilder from mlserve.inputs import BasicInput from mlserve.loader import load_mlflow_model from mlserve.predictions import GenericPrediction # load model model = load_mlflow_model ( # MlFlow model path \"models:/sklearn_model/1\" , # MlFlow Tracking URI \"http://localhost:5000\" , ) # Implement deserializer for input data class WineComposition ( BasicInput ): alcohol : float chlorides : float citric_acid : float density : float fixed_acidity : float free_sulfur_dioxide : int pH : float residual_sugar : float sulphates : float total_sulfur_dioxide : int volatile_acidity : int # implement application app = ApiBuilder ( GenericPrediction ( model ), WineComposition ) . build_api () To run it : uvicorn examples.serving.sklearn:app --host 0 .0.0.0","title":"Sklearn"},{"location":"examples/sklearn/#training","text":"\"\"\" Example taken from https://github.com/mlflow/mlflow/blob/master/examples/sklearn_elasticnet_wine/train.py \"\"\" import warnings import sys import pandas as pd import numpy as np from sklearn.metrics import mean_squared_error , mean_absolute_error , r2_score from sklearn.model_selection import train_test_split from sklearn.linear_model import ElasticNet import mlflow import mlflow.sklearn import logging logging . basicConfig ( level = logging . WARN ) logger = logging . getLogger ( __name__ ) def eval_metrics ( actual , pred ): rmse = np . sqrt ( mean_squared_error ( actual , pred )) mae = mean_absolute_error ( actual , pred ) r2 = r2_score ( actual , pred ) return rmse , mae , r2 if __name__ == \"__main__\" : mlflow . set_tracking_uri ( \"http://localhost:5000\" ) warnings . filterwarnings ( \"ignore\" ) np . random . seed ( 40 ) # Read the wine-quality csv file from the URL csv_url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\" try : data = pd . read_csv ( csv_url , sep = \";\" ) except Exception as e : logger . exception ( \"Unable to download training & test CSV, check your internet connection. Error: %s \" , e , ) # Split the data into training and test sets. (0.75, 0.25) split. train , test = train_test_split ( data ) # The predicted column is \"quality\" which is a scalar from [3, 9] train_x = train . drop ([ \"quality\" ], axis = 1 ) test_x = test . drop ([ \"quality\" ], axis = 1 ) train_y = train [[ \"quality\" ]] test_y = test [[ \"quality\" ]] alpha = float ( sys . argv [ 1 ]) if len ( sys . argv ) > 1 else 0.5 l1_ratio = float ( sys . argv [ 2 ]) if len ( sys . argv ) > 2 else 0.5 with mlflow . start_run ( experiment_id = 0 ): lr = ElasticNet ( alpha = alpha , l1_ratio = l1_ratio , random_state = 42 ) lr . fit ( train_x , train_y ) predicted_qualities = lr . predict ( test_x ) ( rmse , mae , r2 ) = eval_metrics ( test_y , predicted_qualities ) print ( \"Elasticnet model (alpha= %f , l1_ratio= %f ):\" % ( alpha , l1_ratio )) print ( \" RMSE: %s \" % rmse ) print ( \" MAE: %s \" % mae ) print ( \" R2: %s \" % r2 ) mlflow . log_param ( \"alpha\" , alpha ) mlflow . log_param ( \"l1_ratio\" , l1_ratio ) mlflow . log_metric ( \"rmse\" , rmse ) mlflow . log_metric ( \"r2\" , r2 ) mlflow . log_metric ( \"mae\" , mae ) mlflow . sklearn . log_model ( lr , \"model\" , registered_model_name = \"sklearn_model\" ) To run it : python3 -m examples.training.sklearn","title":"Training"},{"location":"examples/sklearn/#serving","text":"from mlserve.api import ApiBuilder from mlserve.inputs import BasicInput from mlserve.loader import load_mlflow_model from mlserve.predictions import GenericPrediction # load model model = load_mlflow_model ( # MlFlow model path \"models:/sklearn_model/1\" , # MlFlow Tracking URI \"http://localhost:5000\" , ) # Implement deserializer for input data class WineComposition ( BasicInput ): alcohol : float chlorides : float citric_acid : float density : float fixed_acidity : float free_sulfur_dioxide : int pH : float residual_sugar : float sulphates : float total_sulfur_dioxide : int volatile_acidity : int # implement application app = ApiBuilder ( GenericPrediction ( model ), WineComposition ) . build_api () To run it : uvicorn examples.serving.sklearn:app --host 0 .0.0.0","title":"Serving"},{"location":"examples/tensorflow/","text":"Training \"\"\" Example taken from https://github.com/mlflow/mlflow/blob/master/examples/tensorflow/tf2/train_predict_2.py \"\"\" from __future__ import absolute_import from __future__ import division from __future__ import print_function import tempfile import mlflow import argparse import pandas as pd import tensorflow as tf import mlflow.tensorflow TRAIN_URL = \"http://download.tensorflow.org/data/iris_training.csv\" TEST_URL = \"http://download.tensorflow.org/data/iris_test.csv\" CSV_COLUMN_NAMES = [ \"SepalLength\" , \"SepalWidth\" , \"PetalLength\" , \"PetalWidth\" , \"Species\" , ] SPECIES = [ \"Setosa\" , \"Versicolor\" , \"Virginica\" ] def load_data ( y_name = \"Species\" ): \"\"\"Returns the iris dataset as (train_x, train_y), (test_x, test_y).\"\"\" train_path = tf . keras . utils . get_file ( TRAIN_URL . split ( \"/\" )[ - 1 ], TRAIN_URL ) test_path = tf . keras . utils . get_file ( TEST_URL . split ( \"/\" )[ - 1 ], TEST_URL ) train = pd . read_csv ( train_path , names = CSV_COLUMN_NAMES , header = 0 ) train_x , train_y = train , train . pop ( y_name ) test = pd . read_csv ( test_path , names = CSV_COLUMN_NAMES , header = 0 ) test_x , test_y = test , test . pop ( y_name ) return ( train_x , train_y ), ( test_x , test_y ) def train_input_fn ( features , labels , batch_size ): \"\"\"An input function for training\"\"\" # Convert the inputs to a Dataset. dataset = tf . data . Dataset . from_tensor_slices (( dict ( features ), labels )) # Shuffle, repeat, and batch the examples. dataset = dataset . shuffle ( 1000 ) . repeat () . batch ( batch_size ) # Return the dataset. return dataset def eval_input_fn ( features , labels , batch_size ): \"\"\"An input function for evaluation or prediction\"\"\" features = dict ( features ) if labels is None : # No labels, use only features. inputs = features else : inputs = ( features , labels ) # Convert the inputs to a Dataset. dataset = tf . data . Dataset . from_tensor_slices ( inputs ) # Batch the examples assert batch_size is not None , \"batch_size must not be None\" dataset = dataset . batch ( batch_size ) # Return the dataset. return dataset # Enable auto-logging to MLflow to capture TensorBoard metrics. mlflow . tensorflow . autolog () parser = argparse . ArgumentParser () parser . add_argument ( \"--batch_size\" , default = 100 , type = int , help = \"batch size\" ) parser . add_argument ( \"--train_steps\" , default = 1000 , type = int , help = \"number of training steps\" ) def main ( args ): with mlflow . start_run ( experiment_id = 4 ): # Fetch the data ( train_x , train_y ), ( test_x , test_y ) = load_data () # Feature columns describe how to use the input. my_feature_columns = [] for key in train_x . keys (): my_feature_columns . append ( tf . feature_column . numeric_column ( key = key ) ) # Two hidden layers of 10 nodes each. hidden_units = [ 10 , 10 ] # Build 2 hidden layer DNN with 10, 10 units respectively. classifier = tf . estimator . DNNClassifier ( feature_columns = my_feature_columns , hidden_units = hidden_units , # The model must choose between 3 classes. n_classes = 3 , ) # Train the Model. classifier . train ( input_fn = lambda : train_input_fn ( train_x , train_y , args . batch_size ), steps = args . train_steps , ) # Evaluate the model. eval_result = classifier . evaluate ( input_fn = lambda : eval_input_fn ( test_x , test_y , args . batch_size ) ) print ( \" \\n Test set accuracy: {accuracy:0.3f} \\n \" . format ( ** eval_result )) # Creating output tf.Variables to specify the output of the saved model. feat_specifications = { \"SepalLength\" : tf . Variable ( [], dtype = tf . float64 , name = \"SepalLength\" ), \"SepalWidth\" : tf . Variable ([], dtype = tf . float64 , name = \"SepalWidth\" ), \"PetalLength\" : tf . Variable ( [], dtype = tf . float64 , name = \"PetalLength\" ), \"PetalWidth\" : tf . Variable ([], dtype = tf . float64 , name = \"PetalWidth\" ), } receiver_fn = tf . estimator . export . build_raw_serving_input_receiver_fn ( feat_specifications ) temp = tempfile . mkdtemp () classifier . export_saved_model ( temp , receiver_fn ,) . decode ( \"utf-8\" ) # custom code for registering models mlflow_client = mlflow . tracking . MlflowClient ( \"http://localhost:5000\" ) run_id = mlflow_client . list_run_infos ( experiment_id = 4 )[ 0 ] . run_id mlflow . register_model ( \"runs:/ {} /artifacts/model\" . format ( run_id ), \"tensorflow_model\" ) if __name__ == \"__main__\" : args = parser . parse_args () main ( args ) To run it : python3 -m examples.training.tensorflow Serving import os import mlflow from mlserve.api import ApiBuilder from mlserve.inputs import BasicInput from mlserve.loader import load_mlflow_model from mlserve.predictions import GenericPrediction # getting run_id mlflow_client = mlflow . tracking . MlflowClient ( \"http://localhost:5000\" ) run_id = mlflow_client . list_run_infos ( experiment_id = 4 )[ 0 ] . run_id current_directory = os . getcwd () model = load_mlflow_model ( \" {} /4/ {} /artifacts/model\" . format ( current_directory , run_id ) ) # Implement deserializer for input data class PetalComposition ( BasicInput ): SepalWidth : float SepalLength : float PetalLength : float PetalWidth : float # implement application app = ApiBuilder ( GenericPrediction ( model ), PetalComposition ) . build_api () To run it : uvicorn examples.serving.tensorflow:app --host 0 .0.0.0","title":"Tensorflow"},{"location":"examples/tensorflow/#training","text":"\"\"\" Example taken from https://github.com/mlflow/mlflow/blob/master/examples/tensorflow/tf2/train_predict_2.py \"\"\" from __future__ import absolute_import from __future__ import division from __future__ import print_function import tempfile import mlflow import argparse import pandas as pd import tensorflow as tf import mlflow.tensorflow TRAIN_URL = \"http://download.tensorflow.org/data/iris_training.csv\" TEST_URL = \"http://download.tensorflow.org/data/iris_test.csv\" CSV_COLUMN_NAMES = [ \"SepalLength\" , \"SepalWidth\" , \"PetalLength\" , \"PetalWidth\" , \"Species\" , ] SPECIES = [ \"Setosa\" , \"Versicolor\" , \"Virginica\" ] def load_data ( y_name = \"Species\" ): \"\"\"Returns the iris dataset as (train_x, train_y), (test_x, test_y).\"\"\" train_path = tf . keras . utils . get_file ( TRAIN_URL . split ( \"/\" )[ - 1 ], TRAIN_URL ) test_path = tf . keras . utils . get_file ( TEST_URL . split ( \"/\" )[ - 1 ], TEST_URL ) train = pd . read_csv ( train_path , names = CSV_COLUMN_NAMES , header = 0 ) train_x , train_y = train , train . pop ( y_name ) test = pd . read_csv ( test_path , names = CSV_COLUMN_NAMES , header = 0 ) test_x , test_y = test , test . pop ( y_name ) return ( train_x , train_y ), ( test_x , test_y ) def train_input_fn ( features , labels , batch_size ): \"\"\"An input function for training\"\"\" # Convert the inputs to a Dataset. dataset = tf . data . Dataset . from_tensor_slices (( dict ( features ), labels )) # Shuffle, repeat, and batch the examples. dataset = dataset . shuffle ( 1000 ) . repeat () . batch ( batch_size ) # Return the dataset. return dataset def eval_input_fn ( features , labels , batch_size ): \"\"\"An input function for evaluation or prediction\"\"\" features = dict ( features ) if labels is None : # No labels, use only features. inputs = features else : inputs = ( features , labels ) # Convert the inputs to a Dataset. dataset = tf . data . Dataset . from_tensor_slices ( inputs ) # Batch the examples assert batch_size is not None , \"batch_size must not be None\" dataset = dataset . batch ( batch_size ) # Return the dataset. return dataset # Enable auto-logging to MLflow to capture TensorBoard metrics. mlflow . tensorflow . autolog () parser = argparse . ArgumentParser () parser . add_argument ( \"--batch_size\" , default = 100 , type = int , help = \"batch size\" ) parser . add_argument ( \"--train_steps\" , default = 1000 , type = int , help = \"number of training steps\" ) def main ( args ): with mlflow . start_run ( experiment_id = 4 ): # Fetch the data ( train_x , train_y ), ( test_x , test_y ) = load_data () # Feature columns describe how to use the input. my_feature_columns = [] for key in train_x . keys (): my_feature_columns . append ( tf . feature_column . numeric_column ( key = key ) ) # Two hidden layers of 10 nodes each. hidden_units = [ 10 , 10 ] # Build 2 hidden layer DNN with 10, 10 units respectively. classifier = tf . estimator . DNNClassifier ( feature_columns = my_feature_columns , hidden_units = hidden_units , # The model must choose between 3 classes. n_classes = 3 , ) # Train the Model. classifier . train ( input_fn = lambda : train_input_fn ( train_x , train_y , args . batch_size ), steps = args . train_steps , ) # Evaluate the model. eval_result = classifier . evaluate ( input_fn = lambda : eval_input_fn ( test_x , test_y , args . batch_size ) ) print ( \" \\n Test set accuracy: {accuracy:0.3f} \\n \" . format ( ** eval_result )) # Creating output tf.Variables to specify the output of the saved model. feat_specifications = { \"SepalLength\" : tf . Variable ( [], dtype = tf . float64 , name = \"SepalLength\" ), \"SepalWidth\" : tf . Variable ([], dtype = tf . float64 , name = \"SepalWidth\" ), \"PetalLength\" : tf . Variable ( [], dtype = tf . float64 , name = \"PetalLength\" ), \"PetalWidth\" : tf . Variable ([], dtype = tf . float64 , name = \"PetalWidth\" ), } receiver_fn = tf . estimator . export . build_raw_serving_input_receiver_fn ( feat_specifications ) temp = tempfile . mkdtemp () classifier . export_saved_model ( temp , receiver_fn ,) . decode ( \"utf-8\" ) # custom code for registering models mlflow_client = mlflow . tracking . MlflowClient ( \"http://localhost:5000\" ) run_id = mlflow_client . list_run_infos ( experiment_id = 4 )[ 0 ] . run_id mlflow . register_model ( \"runs:/ {} /artifacts/model\" . format ( run_id ), \"tensorflow_model\" ) if __name__ == \"__main__\" : args = parser . parse_args () main ( args ) To run it : python3 -m examples.training.tensorflow","title":"Training"},{"location":"examples/tensorflow/#serving","text":"import os import mlflow from mlserve.api import ApiBuilder from mlserve.inputs import BasicInput from mlserve.loader import load_mlflow_model from mlserve.predictions import GenericPrediction # getting run_id mlflow_client = mlflow . tracking . MlflowClient ( \"http://localhost:5000\" ) run_id = mlflow_client . list_run_infos ( experiment_id = 4 )[ 0 ] . run_id current_directory = os . getcwd () model = load_mlflow_model ( \" {} /4/ {} /artifacts/model\" . format ( current_directory , run_id ) ) # Implement deserializer for input data class PetalComposition ( BasicInput ): SepalWidth : float SepalLength : float PetalLength : float PetalWidth : float # implement application app = ApiBuilder ( GenericPrediction ( model ), PetalComposition ) . build_api () To run it : uvicorn examples.serving.tensorflow:app --host 0 .0.0.0","title":"Serving"},{"location":"examples/xgboost/","text":"Training \"\"\" Example taken from https://github.com/mlflow/mlflow/blob/master/examples/xgboost/train.py \"\"\" import argparse from sklearn import datasets from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score , log_loss import xgboost as xgb import mlflow import mlflow.xgboost def parse_args (): parser = argparse . ArgumentParser ( description = \"XGBoost example\" ) parser . add_argument ( \"--colsample-bytree\" , type = float , default = 1.0 , help = \"subsample ratio of columns when constructing each tree (default: 1.0)\" , ) parser . add_argument ( \"--subsample\" , type = float , default = 1.0 , help = \"subsample ratio of the training instances (default: 1.0)\" , ) return parser . parse_args () def main (): # parse command-line arguments args = parse_args () # prepare train and test data iris = datasets . load_iris () X = iris . data y = iris . target X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 , random_state = 42 ) dtrain = xgb . DMatrix ( X_train , label = y_train ) dtest = xgb . DMatrix ( X_test , label = y_test ) with mlflow . start_run ( experiment_id = 3 ): # train model params = { \"objective\" : \"multi:softprob\" , \"num_class\" : 3 , \"eval_metric\" : \"mlogloss\" , \"colsample_bytree\" : args . colsample_bytree , \"subsample\" : args . subsample , \"seed\" : 42 , } model = xgb . train ( params , dtrain , evals = [( dtrain , \"train\" )]) # evaluate model y_proba = model . predict ( dtest ) y_pred = y_proba . argmax ( axis = 1 ) loss = log_loss ( y_test , y_proba ) acc = accuracy_score ( y_test , y_pred ) # log metrics mlflow . log_metrics ({ \"log_loss\" : loss , \"accuracy\" : acc }) mlflow . xgboost . log_model ( model , \"model\" , registered_model_name = \"xgboost_model\" ) if __name__ == \"__main__\" : main () To run it : python3 -m examples.training.xgboost Serving from mlserve.api import ApiBuilder from mlserve.inputs import BasicInput from mlserve.loader import load_mlflow_model from mlserve.predictions import GenericPrediction # load model model = load_mlflow_model ( # MlFlow model path \"models:/xgboost_model/1\" , # MlFlow Tracking URI \"http://localhost:5000\" , ) # Implement deserializer for input data class PetalComposition ( BasicInput ): sepal_length : float sepal_width : float petal_length : float petal_width : float # implement application app = ApiBuilder ( GenericPrediction ( model ), PetalComposition ) . build_api () To run it : uvicorn examples.serving.xgboost:app --host 0 .0.0.0","title":"Xgboost"},{"location":"examples/xgboost/#training","text":"\"\"\" Example taken from https://github.com/mlflow/mlflow/blob/master/examples/xgboost/train.py \"\"\" import argparse from sklearn import datasets from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score , log_loss import xgboost as xgb import mlflow import mlflow.xgboost def parse_args (): parser = argparse . ArgumentParser ( description = \"XGBoost example\" ) parser . add_argument ( \"--colsample-bytree\" , type = float , default = 1.0 , help = \"subsample ratio of columns when constructing each tree (default: 1.0)\" , ) parser . add_argument ( \"--subsample\" , type = float , default = 1.0 , help = \"subsample ratio of the training instances (default: 1.0)\" , ) return parser . parse_args () def main (): # parse command-line arguments args = parse_args () # prepare train and test data iris = datasets . load_iris () X = iris . data y = iris . target X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 , random_state = 42 ) dtrain = xgb . DMatrix ( X_train , label = y_train ) dtest = xgb . DMatrix ( X_test , label = y_test ) with mlflow . start_run ( experiment_id = 3 ): # train model params = { \"objective\" : \"multi:softprob\" , \"num_class\" : 3 , \"eval_metric\" : \"mlogloss\" , \"colsample_bytree\" : args . colsample_bytree , \"subsample\" : args . subsample , \"seed\" : 42 , } model = xgb . train ( params , dtrain , evals = [( dtrain , \"train\" )]) # evaluate model y_proba = model . predict ( dtest ) y_pred = y_proba . argmax ( axis = 1 ) loss = log_loss ( y_test , y_proba ) acc = accuracy_score ( y_test , y_pred ) # log metrics mlflow . log_metrics ({ \"log_loss\" : loss , \"accuracy\" : acc }) mlflow . xgboost . log_model ( model , \"model\" , registered_model_name = \"xgboost_model\" ) if __name__ == \"__main__\" : main () To run it : python3 -m examples.training.xgboost","title":"Training"},{"location":"examples/xgboost/#serving","text":"from mlserve.api import ApiBuilder from mlserve.inputs import BasicInput from mlserve.loader import load_mlflow_model from mlserve.predictions import GenericPrediction # load model model = load_mlflow_model ( # MlFlow model path \"models:/xgboost_model/1\" , # MlFlow Tracking URI \"http://localhost:5000\" , ) # Implement deserializer for input data class PetalComposition ( BasicInput ): sepal_length : float sepal_width : float petal_length : float petal_width : float # implement application app = ApiBuilder ( GenericPrediction ( model ), PetalComposition ) . build_api () To run it : uvicorn examples.serving.xgboost:app --host 0 .0.0.0","title":"Serving"}]}